{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of LSTM\n",
    "## Variables to Consider\n",
    "<ul>\n",
    "    <li>Hidden Parameters (more seems better until 10)</li>\n",
    "    <li>Hidden Layers (more than 1 seems useless, makes sense because input is few dimensions)</li>\n",
    "    <li>Learning Rate (smaller learning rate requires more epoches to achieve similar results)</li>\n",
    "    <li>Mini-Batch Size (seems to have slight improvement)</li>\n",
    "    <li>Number Epochs</li>\n",
    "    <li>Sequence Length (4)</li>\n",
    "    <li>Length of Prediction (1)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13538,
     "status": "error",
     "timestamp": 1590900514206,
     "user": {
      "displayName": "Langston Nashold",
      "photoUrl": "",
      "userId": "03058483142038316977"
     },
     "user_tz": 300
    },
    "id": "VAzzi5jKnjvJ",
    "outputId": "42fe0153-d955-48ba-c761-2319f3dfad81"
   },
   "outputs": [],
   "source": [
    "#!pip uninstall statsmodels\n",
    "#!pip install pmdarima\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"using cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1590900464686,
     "user": {
      "displayName": "Langston Nashold",
      "photoUrl": "",
      "userId": "03058483142038316977"
     },
     "user_tz": 300
    },
    "id": "DzEKHrLmtEYI",
    "outputId": "b523f4cb-6c47-440e-89a7-0df85cf4fb02"
   },
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    def __init__(self, num_layers = 1, hidden_size = 10, batch_size = 100, seq_len = 6, pred_len = 3, num_epochs = 20, learning_rate = 0.01, dropout = 1):\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"{}-{}-{}-{}-{}-{}-{}-{}\".format(self.num_layers, self.hidden_size, self.batch_size, self.seq_len, self.pred_len, self.num_epochs, self.learning_rate, self.dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kXV_VIWvO-he"
   },
   "source": [
    "Following Code Adapted from ([github sample](https://github.com/spdin/time-series-prediction-lstm-pytorch/blob/master/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 1)\n"
     ]
    }
   ],
   "source": [
    "def readData(file='samples/WY.csv', freq=20):\n",
    "    data = pd.read_csv(file, delimiter=',', index_col=0, parse_dates=True)\n",
    "\n",
    "    #plt.figure(figsize=(20, 4))\n",
    "    #plt.plot(data)\n",
    "    #print(data)\n",
    "    data = data.resample(str(freq) + 'T').mean()\n",
    "    raw_values = np.asarray(data['CpuUtilizationAverage']).reshape(data.shape[0], 1)\n",
    "    #plt.plot(data)\n",
    "    #plt.show()\n",
    "    #print(raw_values)\n",
    "    return raw_values\n",
    "\n",
    "training_set = readData()\n",
    "print(training_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ESj__1lcPQzg",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-d8d91b4faf66>:51: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if state_name==state:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1072, 3)\n",
      "torch.Size([800, 6, 50])\n",
      "torch.Size([800, 3])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# just used to store some data variables\n",
    "class Data:\n",
    "    pass\n",
    "\n",
    "def large_sliding_windows(data, seq_length, pred_length=2):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-seq_length-pred_length+1):\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[i+seq_length:i+seq_length+pred_length]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x),np.array(y)\n",
    "\n",
    "def sliding_windows(data, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[i+seq_length]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x),np.array(y)\n",
    "\n",
    "def get_concurrent():\n",
    "    data = np.zeros((50, 21600))\n",
    "    for i, state in enumerate(listdir(\"samples/\")):\n",
    "        training_set = readData(\"samples/\"+state)\n",
    "        training_set = training_set.flatten()\n",
    "        data[i] = training_set\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(21600-1):\n",
    "        _x = data[:,i:(i+1)]\n",
    "        _y = data[:,i+1]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "def get_concurrent_features(seq_len, pred_len, state_name):\n",
    "    data = np.zeros((1080, 50)) #fix hard coding\n",
    "    state_id=0\n",
    "    for i, state in enumerate(listdir(\"samples/\")):\n",
    "        if state_name==state:\n",
    "            state_id=i\n",
    "            print(\"using state {} with id {}\".format(state, i))\n",
    "        training_set = readData(\"samples/\"+state)\n",
    "        training_set = training_set.flatten()\n",
    "        for j in range(data.shape[0]):\n",
    "            data[j][i] = training_set[j]\n",
    "    x, y = large_sliding_windows(data, seq_len, pred_len)\n",
    "    y = y[:,:,state_id]\n",
    "    print(y.shape)\n",
    "    return x, y\n",
    "    \n",
    "\n",
    "def generateData(state_name, d, params):\n",
    "    d.sc = MinMaxScaler()\n",
    "    training_data = d.sc.fit_transform(training_set) # normalizes the data\n",
    "\n",
    "    seq_length = params.seq_len # parameter\n",
    "    pred_len = params.pred_len # parameter\n",
    "    x, y = get_concurrent_features(seq_length, pred_len, state_name)\n",
    "\n",
    "    batch_size= params.batch_size\n",
    "    d.max_size = int(batch_size*(len(y)//batch_size))\n",
    "    d.train_size = int(batch_size*(d.max_size*(0.8)//batch_size)) #train_size = int(len(y) * 0.67)\n",
    "    d.test_size = d.max_size-d.train_size\n",
    "    #test_size = len(y) - train_size\n",
    "\n",
    "    d.dataX = Variable(torch.Tensor(np.array(x[0:d.max_size])))\n",
    "    d.dataY = Variable(torch.Tensor(np.array(y[0:d.max_size])))\n",
    "\n",
    "    d.trainX = Variable(torch.Tensor(np.array(x[0:d.train_size]))).to(device)\n",
    "    d.trainY = Variable(torch.Tensor(np.array(y[0:d.train_size]))).to(device)\n",
    "    train_data = TensorDataset(d.trainX, d.trainY)\n",
    "    d.train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size) #primary\n",
    "\n",
    "    d.testX = Variable(torch.Tensor(np.array(x[d.train_size:d.max_size]))).to(device)\n",
    "    d.testY = Variable(torch.Tensor(np.array(y[d.train_size:d.max_size]))).to(device)\n",
    "    #test_data = TensorDataset(testX, testY)\n",
    "    test_data = TensorDataset(d.dataX, d.dataY)\n",
    "    d.test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size) #primary\n",
    "    return d\n",
    "data = Data()\n",
    "params = Parameters()\n",
    "generateData(training_set, data, params)\n",
    "print(data.trainX.shape)\n",
    "print(data.trainY.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2e1ebc2ec806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mglobal_naive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_naive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mnaives\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgenerateNaive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-2e1ebc2ec806>\u001b[0m in \u001b[0;36mgenerateNaive\u001b[0;34m(d, params)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmean_forecasts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnaive_forecasts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# always predicts last provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfour_naive_forecasts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfour_naive_forecasts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "## Generate Naive Examples\n",
    "\n",
    "def generateNaive(d, params):\n",
    "    mean = d.trainY.mean()\n",
    "    mean_forecasts = np.full(d.testY.shape, mean.item())\n",
    "    naive_forecasts = np.full(d.testY.shape, d.trainX[-1][-1].item())  # always predicts last provided\n",
    "    four_naive_forecasts = np.ones(d.testY.shape)\n",
    "    for row in four_naive_forecasts:\n",
    "        for i in range(params.pred_len):\n",
    "            row[i] = d.testX[i][-1].item()\n",
    "    four_mean_forecasts = np.ones(d.testY.shape)\n",
    "    for row, test_row in zip(four_mean_forecasts, d.testX):\n",
    "        for i in range(params.pred_len):\n",
    "            row[i] = np.mean(test_row.numpy())\n",
    "\n",
    "    #naive_forecasts = naive_forecasts.reshape(-1, 1)\n",
    "    #mean_forecasts = mean_forecasts.reshape(-1, 1)\n",
    "\n",
    "    denormalize=False\n",
    "\n",
    "    if denormalize:\n",
    "        naive_forecasts = sc.inverse_transform(naive_forecasts)\n",
    "        mean_forecasts = sc.inverse_transform(mean_forecasts)\n",
    "        four_naive_forecasts = sc.inverse_transform(four_naive_forecasts)\n",
    "        four_mean_forecasts = sc.inverse_transform(four_mean_forecasts)\n",
    "\n",
    "\n",
    "    global_naive = naive_forecasts.reshape(naive_forecasts.shape[:-2] + (-1,))\n",
    "    global_mean = mean_forecasts.reshape(mean_forecasts.shape[:-2] + (-1,))\n",
    "    local_naive = four_naive_forecasts.reshape(four_naive_forecasts.shape[:-2] + (-1,))\n",
    "    local_mean = four_mean_forecasts.reshape(four_mean_forecasts.shape[:-2] + (-1,))\n",
    "    return (global_naive, global_mean, local_naive, local_mean)\n",
    "\n",
    "naives  = generateNaive(data, params)\n",
    "print(data.testX.shape)\n",
    "print(data.testY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3dTo7J9PWI5"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, output_size, input_size, hidden_dim, n_layers, drop_prob=0):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        #x = x.long()\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        #print(lstm_out.shape)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        #out = self.dropout(lstm_out)\n",
    "        #print(lstm_out.shape)\n",
    "        #out = self.fc(lstm_out) # linear transform from hidden dims to output_size\n",
    "        #print(out.shape)\n",
    "        #out = out.view(batch_size, -1) #-1 means size will be infered\n",
    "        #print(out.shape)\n",
    "        #out = out[:,-2]\n",
    "        #print(out.shape)\n",
    "        #print(hidden[0][-1].shape)\n",
    "        h_out = hidden[0][-1].view(-1, self.hidden_dim)\n",
    "        out = self.fc(h_out)\n",
    "        #print(out.shape)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9AO0JvNKPZcE"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ARlW89KPanR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rayan/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:47: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 151.96112\n",
      "Epoch: 10, loss: 44.12098\n"
     ]
    }
   ],
   "source": [
    "class Model:\n",
    "    def __init__(self, lstm, h, criterion):\n",
    "        self.lstm, self.h, self.criterion = lstm, h, criterion\n",
    "\n",
    "def train(d, params):\n",
    "    num_epochs = params.num_epochs#30 # seems to stabilize here, higher and it seems to overfit\n",
    "    learning_rate = params.learning_rate#0.01\n",
    "\n",
    "    input_size = 50 # required for this time series (value)\n",
    "    hidden_size = params.hidden_size#2 # hyper parameter\n",
    "    num_layers = params.num_layers#1 # hyper parameter\n",
    "\n",
    "    lstm = LSTM(params.pred_len, input_size, hidden_size, num_layers, params.dropout)\n",
    "    lstm.to(device)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "    #criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "    #optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "    h = lstm.init_hidden(params.batch_size)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, y in d.train_loader:\n",
    "            #x = x.to(device)\n",
    "            #y = y.to(device)\n",
    "            h = tuple([e.data for e in h])\n",
    "            outputs, h = lstm(x, h)\n",
    "            optimizer.zero_grad() # remove stored gradient\n",
    "\n",
    "            # obtain the loss function\n",
    "            #print(outputs.shape)\n",
    "            #print(outputs.shape)\n",
    "            #print(y.shape)\n",
    "            loss = criterion(outputs, y) #.squeeze()\n",
    "            #print(outputs)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "            #pass\n",
    "            \n",
    "    model = Model(lstm, h, criterion)\n",
    "    return model\n",
    "            \n",
    "model = train(data, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUVXq6eYPbnx"
   },
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nc8WL0XuPcxa"
   },
   "outputs": [],
   "source": [
    "def test(model, d, params, show_plot=False):\n",
    "    model.lstm.eval()\n",
    "    test_predict = np.zeros((data.max_size, params.pred_len))\n",
    "    test_losses = []\n",
    "\n",
    "    for i, (x, y) in enumerate(d.test_loader):\n",
    "        model.h = tuple([each.data for each in model.h])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output, model.h = model.lstm(x, model.h)\n",
    "        test_loss = model.criterion(output.squeeze(), y)\n",
    "        test_losses.append(test_loss.item())\n",
    "        for j in range(params.batch_size):\n",
    "            #test_predict.append(output[j].item())\n",
    "            #test_predict[i*100+j] = output[j].item()\n",
    "            for k in range(params.pred_len):\n",
    "                test_predict[j+i*100][k] = output[j][k].item()\n",
    "\n",
    "\n",
    "    #print(test_predict)\n",
    "\n",
    "    #data_predict = test_predict.reshape(-1, 1)#test_predict.data.numpy()\n",
    "    data_predict = test_predict\n",
    "    dataY_plot = d.dataY.data.numpy()\n",
    "    \n",
    "    data_predict_transform = d.sc.inverse_transform(data_predict)\n",
    "    dataY_plot_transform = d.sc.inverse_transform(dataY_plot)\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.figure(figsize=(20, 4))\n",
    "        plt.axvline(x=d.train_size, c='r', linestyle='--') # data shift from train to test\n",
    "        plt.plot(dataY_plot_transform)\n",
    "        plt.plot(data_predict_transform)\n",
    "        plt.suptitle('Time-Series Prediction')\n",
    "        plt.show()\n",
    "\n",
    "    #print(data_predict_transform)\n",
    "    #print(dataY_plot_transform)\n",
    "    return (data_predict[d.train_size:], dataY_plot[d.train_size:])\n",
    "\n",
    "preds, labels = test(model, data, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GYO9JliaG2g"
   },
   "source": [
    "Image is deceptive. Try ~100 samples to see a closer fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JAA4fI6vaLvg"
   },
   "source": [
    "Accuracy Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rISXuL-3I1Bl"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (215000,) (600,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b4a49605bd85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnaive\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnaives\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mprint_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-b4a49605bd85>\u001b[0m in \u001b[0;36mprint_errors\u001b[0;34m(preds, labels, prnt)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0merr_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32min\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0merr_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merr_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-b4a49605bd85>\u001b[0m in \u001b[0;36mmae\u001b[0;34m(preds, labels, prnt)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprnt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MAE - {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (215000,) (600,) "
     ]
    }
   ],
   "source": [
    "eps = 1e-5\n",
    "# Mean Average Percent Error\n",
    "def mape(preds, labels, prnt=False):\n",
    "    preds = preds.flatten()\n",
    "    labels = labels.flatten()\n",
    "    err = 0\n",
    "    for i, (pred, label) in enumerate(zip(preds, labels)):\n",
    "        denum = np.absolute(label) if np.round(label) !=0 else 50#np.max(labels) # this might be wrong\n",
    "        err += (np.absolute(pred-label) / denum)\n",
    "            \n",
    "    err /= preds.shape[0]\n",
    "    if prnt: print(\"MAPE - {}\".format(err))\n",
    "    return err\n",
    "    \n",
    "# Brier Score or Mean Squared Error\n",
    "def mse(preds, labels, prnt=False):\n",
    "    preds = preds.flatten()\n",
    "    labels = labels.flatten()\n",
    "    err = np.sum(np.power(preds-labels, 2)) / preds.shape[0]\n",
    "    if prnt: print(\"MSE - {}\".format(err))\n",
    "    return err\n",
    "    \n",
    "def mae(preds, labels, prnt=False):\n",
    "    preds = preds.flatten()\n",
    "    labels = labels.flatten()\n",
    "    err = (np.sum(np.absolute(preds-labels))) / preds.shape[0]\n",
    "    if prnt: print(\"MAE - {}\".format(err))\n",
    "    return err\n",
    "    \n",
    "# Root Mean Squared Error\n",
    "def rmse(preds, labels, prnt=False):\n",
    "    err = np.power(mse(preds, labels), 0.5)\n",
    "    if prnt: print(\"RMSE - {}\".format(err))\n",
    "    return err\n",
    "\n",
    "# Symmetric Mean Absolute Percentage Error\n",
    "# some issues in bias, but commonly used\n",
    "def smape(preds, labels, prnt=False):\n",
    "    preds = preds.flatten()\n",
    "    labels = labels.flatten()\n",
    "    err = 0\n",
    "    for (pred, label) in zip(preds, labels):\n",
    "        denum = np.absolute(pred)+np.absolute(label) if np.absolute(pred)+np.absolute(label) !=0 else np.max(labels) #check!!\n",
    "        err += (np.absolute(pred-label) / denum)\n",
    "    err /= preds.shape[0] # in textbook, also multiply by 200 but this might be for percentage?\n",
    "    if prnt: print(\"SMAPE - {}\".format(err))\n",
    "    return err\n",
    "\n",
    "#errors = [mape, mse, rmse, smape]\n",
    "errors = [mae, rmse, mape]\n",
    "\n",
    "def print_errors(preds, labels, prnt=False):\n",
    "    err_results = []\n",
    "    for error in errors:\n",
    "        err_results.append(error(preds, labels, prnt))\n",
    "    return err_results\n",
    "        \n",
    "print_errors(preds, labels)\n",
    "\n",
    "for naive in naives:\n",
    "\n",
    "    print_errors(naive, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using state ND.csv with id 0\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 150.68330\n",
      "Epoch: 10, loss: 13.03061\n",
      "Epoch: 20, loss: 2.67542\n",
      "Epoch: 30, loss: 2.42016\n",
      "Epoch: 40, loss: 2.41325\n",
      "0\n",
      "using state KY.csv with id 1\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 38.53774\n",
      "Epoch: 10, loss: 0.98634\n",
      "Epoch: 20, loss: 0.90574\n",
      "Epoch: 30, loss: 0.90399\n",
      "Epoch: 40, loss: 0.90436\n",
      "1\n",
      "using state RI.csv with id 2\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 122.97905\n",
      "Epoch: 10, loss: 47.82233\n",
      "Epoch: 20, loss: 46.19347\n",
      "Epoch: 30, loss: 46.17688\n",
      "Epoch: 40, loss: 36.93348\n",
      "2\n",
      "using state CA.csv with id 3\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 313.55908\n",
      "Epoch: 10, loss: 147.62901\n",
      "Epoch: 20, loss: 124.51994\n",
      "Epoch: 30, loss: 121.54853\n",
      "Epoch: 40, loss: 121.18435\n",
      "3\n",
      "using state DE.csv with id 4\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 460.85693\n",
      "Epoch: 10, loss: 222.34993\n",
      "Epoch: 20, loss: 165.13432\n",
      "Epoch: 30, loss: 152.42097\n",
      "Epoch: 40, loss: 149.85263\n",
      "4\n",
      "using state OK.csv with id 5\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 215.63779\n",
      "Epoch: 10, loss: 142.03969\n",
      "Epoch: 20, loss: 134.06017\n",
      "Epoch: 30, loss: 96.63786\n",
      "Epoch: 40, loss: 60.21392\n",
      "5\n",
      "using state NE.csv with id 6\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 243.72206\n",
      "Epoch: 10, loss: 79.80406\n",
      "Epoch: 20, loss: 56.62109\n",
      "Epoch: 30, loss: 54.26127\n",
      "Epoch: 40, loss: 54.08880\n",
      "6\n",
      "using state MD.csv with id 7\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 3959.74561\n",
      "Epoch: 10, loss: 3215.46313\n",
      "Epoch: 20, loss: 2767.78418\n",
      "Epoch: 30, loss: 2473.28125\n",
      "Epoch: 40, loss: 2279.97974\n",
      "7\n",
      "using state MA.csv with id 8\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 114.79352\n",
      "Epoch: 10, loss: 61.08534\n",
      "Epoch: 20, loss: 60.80964\n",
      "Epoch: 30, loss: 45.07748\n",
      "Epoch: 40, loss: 30.49099\n",
      "8\n",
      "using state ID.csv with id 9\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 213.31918\n",
      "Epoch: 10, loss: 131.20918\n",
      "Epoch: 20, loss: 107.49468\n",
      "Epoch: 30, loss: 41.61065\n",
      "Epoch: 40, loss: 19.14070\n",
      "9\n",
      "using state VA.csv with id 10\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 2.54596\n",
      "Epoch: 10, loss: 22.73453\n",
      "Epoch: 20, loss: 53.95797\n",
      "Epoch: 30, loss: 72.14917\n",
      "Epoch: 40, loss: 81.35642\n",
      "10\n",
      "using state TX.csv with id 11\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 1867.50024\n",
      "Epoch: 10, loss: 1305.24634\n",
      "Epoch: 20, loss: 1038.77148\n",
      "Epoch: 30, loss: 903.91833\n",
      "Epoch: 40, loss: 836.48505\n",
      "11\n",
      "using state CT.csv with id 12\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 677.68494\n",
      "Epoch: 10, loss: 460.92737\n",
      "Epoch: 20, loss: 409.16669\n",
      "Epoch: 30, loss: 396.24713\n",
      "Epoch: 40, loss: 314.09744\n",
      "12\n",
      "using state MO.csv with id 13\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 192.40056\n",
      "Epoch: 10, loss: 74.53628\n",
      "Epoch: 20, loss: 52.99356\n",
      "Epoch: 30, loss: 49.47045\n",
      "Epoch: 40, loss: 48.91344\n",
      "13\n",
      "using state GA.csv with id 14\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 1172.70850\n",
      "Epoch: 10, loss: 889.80865\n",
      "Epoch: 20, loss: 787.23199\n",
      "Epoch: 30, loss: 750.91718\n",
      "Epoch: 40, loss: 738.34302\n",
      "14\n",
      "using state IL.csv with id 15\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 149.11670\n",
      "Epoch: 10, loss: 105.35435\n",
      "Epoch: 20, loss: 88.18155\n",
      "Epoch: 30, loss: 61.84451\n",
      "Epoch: 40, loss: 48.79196\n",
      "15\n",
      "using state NJ.csv with id 16\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 1562.44666\n",
      "Epoch: 10, loss: 1222.52966\n",
      "Epoch: 20, loss: 1101.22510\n",
      "Epoch: 30, loss: 1059.06506\n",
      "Epoch: 40, loss: 1044.38916\n",
      "16\n",
      "using state AZ.csv with id 17\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 728.98303\n",
      "Epoch: 10, loss: 629.38275\n",
      "Epoch: 20, loss: 621.11206\n",
      "Epoch: 30, loss: 573.14001\n",
      "Epoch: 40, loss: 507.36209\n",
      "17\n",
      "using state AL.csv with id 18\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 87.51850\n",
      "Epoch: 10, loss: 74.69196\n",
      "Epoch: 20, loss: 70.00854\n",
      "Epoch: 30, loss: 57.72423\n",
      "Epoch: 40, loss: 44.95239\n",
      "18\n",
      "using state AR.csv with id 19\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 106.87318\n",
      "Epoch: 10, loss: 67.88434\n",
      "Epoch: 20, loss: 55.13638\n",
      "Epoch: 30, loss: 35.27405\n",
      "Epoch: 40, loss: 28.64886\n",
      "19\n",
      "using state WV.csv with id 20\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 991.78400\n",
      "Epoch: 10, loss: 564.91223\n",
      "Epoch: 20, loss: 383.94708\n",
      "Epoch: 30, loss: 306.40320\n",
      "Epoch: 40, loss: 274.71710\n",
      "20\n",
      "using state MI.csv with id 21\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 229.64537\n",
      "Epoch: 10, loss: 71.98681\n",
      "Epoch: 20, loss: 51.54762\n",
      "Epoch: 30, loss: 49.27250\n",
      "Epoch: 40, loss: 49.02810\n",
      "21\n",
      "using state NY.csv with id 22\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 366.69055\n",
      "Epoch: 10, loss: 141.58015\n",
      "Epoch: 20, loss: 87.72676\n",
      "Epoch: 30, loss: 86.40914\n",
      "Epoch: 40, loss: 95.08479\n",
      "22\n",
      "using state TN.csv with id 23\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 336.39896\n",
      "Epoch: 10, loss: 178.38094\n",
      "Epoch: 20, loss: 156.09909\n",
      "Epoch: 30, loss: 153.21811\n",
      "Epoch: 40, loss: 152.77963\n",
      "23\n",
      "using state AK.csv with id 24\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 2207.61230\n",
      "Epoch: 10, loss: 1358.48523\n",
      "Epoch: 20, loss: 826.00940\n",
      "Epoch: 30, loss: 482.66705\n",
      "Epoch: 40, loss: 268.46317\n",
      "24\n",
      "using state OR.csv with id 25\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 445.54007\n",
      "Epoch: 10, loss: 356.07941\n",
      "Epoch: 20, loss: 333.89703\n",
      "Epoch: 30, loss: 279.61722\n",
      "Epoch: 40, loss: 253.68886\n",
      "25\n",
      "using state NV.csv with id 26\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 4325.15967\n",
      "Epoch: 10, loss: 3244.65332\n",
      "Epoch: 20, loss: 2456.90918\n",
      "Epoch: 30, loss: 1843.98718\n",
      "Epoch: 40, loss: 1370.25610\n",
      "26\n",
      "using state KS.csv with id 27\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 61.25449\n",
      "Epoch: 10, loss: 31.72603\n",
      "Epoch: 20, loss: 31.85670\n",
      "Epoch: 30, loss: 31.67803\n",
      "Epoch: 40, loss: 30.37113\n",
      "27\n",
      "using state NC.csv with id 28\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 3885.39258\n",
      "Epoch: 10, loss: 3089.87793\n",
      "Epoch: 20, loss: 2625.81885\n",
      "Epoch: 30, loss: 2342.19604\n",
      "Epoch: 40, loss: 2167.16064\n",
      "28\n",
      "using state ME.csv with id 29\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 123.38239\n",
      "Epoch: 10, loss: 73.97655\n",
      "Epoch: 20, loss: 73.01733\n",
      "Epoch: 30, loss: 73.86558\n",
      "Epoch: 40, loss: 73.79018\n",
      "29\n",
      "using state WY.csv with id 30\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 117.26199\n",
      "Epoch: 10, loss: 16.33590\n",
      "Epoch: 20, loss: 12.38945\n",
      "Epoch: 30, loss: 12.34836\n",
      "Epoch: 40, loss: 12.34048\n",
      "30\n",
      "using state MN.csv with id 31\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 3542.01050\n",
      "Epoch: 10, loss: 2562.97729\n",
      "Epoch: 20, loss: 1942.95447\n",
      "Epoch: 30, loss: 1497.83618\n",
      "Epoch: 40, loss: 1177.14331\n",
      "31\n",
      "using state VT.csv with id 32\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 176.13931\n",
      "Epoch: 10, loss: 127.03709\n",
      "Epoch: 20, loss: 98.37129\n",
      "Epoch: 30, loss: 60.76028\n",
      "Epoch: 40, loss: 53.36376\n",
      "32\n",
      "using state WA.csv with id 33\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 323.65942\n",
      "Epoch: 10, loss: 106.61323\n",
      "Epoch: 20, loss: 46.15316\n",
      "Epoch: 30, loss: 32.57630\n",
      "Epoch: 40, loss: 12.41098\n",
      "33\n",
      "using state NH.csv with id 34\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 122.54185\n",
      "Epoch: 10, loss: 41.88740\n",
      "Epoch: 20, loss: 45.35485\n",
      "Epoch: 30, loss: 50.76578\n",
      "Epoch: 40, loss: 52.12760\n",
      "34\n",
      "using state CO.csv with id 35\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 273.32294\n",
      "Epoch: 10, loss: 195.18814\n",
      "Epoch: 20, loss: 192.85912\n",
      "Epoch: 30, loss: 192.83170\n",
      "Epoch: 40, loss: 192.83920\n",
      "35\n",
      "using state MT.csv with id 36\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 125.15608\n",
      "Epoch: 10, loss: 62.74366\n",
      "Epoch: 20, loss: 60.65171\n",
      "Epoch: 30, loss: 60.20601\n",
      "Epoch: 40, loss: 55.03458\n",
      "36\n",
      "using state NM.csv with id 37\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 187.18834\n",
      "Epoch: 10, loss: 60.34159\n",
      "Epoch: 20, loss: 40.72130\n",
      "Epoch: 30, loss: 37.79882\n",
      "Epoch: 40, loss: 37.27078\n",
      "37\n",
      "using state SD.csv with id 38\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 5878.42236\n",
      "Epoch: 10, loss: 4561.66016\n",
      "Epoch: 20, loss: 3617.08008\n",
      "Epoch: 30, loss: 2882.16138\n",
      "Epoch: 40, loss: 2304.55225\n",
      "38\n",
      "using state MS.csv with id 39\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 46.37638\n",
      "Epoch: 10, loss: 0.60019\n",
      "Epoch: 20, loss: 0.59633\n",
      "Epoch: 30, loss: 0.59322\n",
      "Epoch: 40, loss: 0.59284\n",
      "39\n",
      "using state IA.csv with id 40\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 99.70940\n",
      "Epoch: 10, loss: 32.10049\n",
      "Epoch: 20, loss: 31.88604\n",
      "Epoch: 30, loss: 31.89997\n",
      "Epoch: 40, loss: 29.17747\n",
      "40\n",
      "using state OH.csv with id 41\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 77.08321\n",
      "Epoch: 10, loss: 28.49914\n",
      "Epoch: 20, loss: 28.50876\n",
      "Epoch: 30, loss: 28.51362\n",
      "Epoch: 40, loss: 28.53220\n",
      "41\n",
      "using state PA.csv with id 42\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 650.95270\n",
      "Epoch: 10, loss: 491.24945\n",
      "Epoch: 20, loss: 460.68283\n",
      "Epoch: 30, loss: 455.09427\n",
      "Epoch: 40, loss: 454.00565\n",
      "42\n",
      "using state HI.csv with id 43\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 346.25104\n",
      "Epoch: 10, loss: 120.43610\n",
      "Epoch: 20, loss: 78.20177\n",
      "Epoch: 30, loss: 72.54398\n",
      "Epoch: 40, loss: 72.13769\n",
      "43\n",
      "using state IN.csv with id 44\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 2426.91943\n",
      "Epoch: 10, loss: 1633.79395\n",
      "Epoch: 20, loss: 1175.59534\n",
      "Epoch: 30, loss: 892.70160\n",
      "Epoch: 40, loss: 720.68121\n",
      "44\n",
      "using state WI.csv with id 45\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 1117.76929\n",
      "Epoch: 10, loss: 660.60468\n",
      "Epoch: 20, loss: 458.01068\n",
      "Epoch: 30, loss: 377.14993\n",
      "Epoch: 40, loss: 346.79745\n",
      "45\n",
      "using state FL.csv with id 46\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 509.20251\n",
      "Epoch: 10, loss: 366.02621\n",
      "Epoch: 20, loss: 345.30341\n",
      "Epoch: 30, loss: 342.55408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, loss: 331.77246\n",
      "46\n",
      "using state UT.csv with id 47\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 2415.67578\n",
      "Epoch: 10, loss: 1702.83655\n",
      "Epoch: 20, loss: 1299.95361\n",
      "Epoch: 30, loss: 1056.56775\n",
      "Epoch: 40, loss: 913.46002\n",
      "47\n",
      "using state LA.csv with id 48\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 117.05021\n",
      "Epoch: 10, loss: 50.56598\n",
      "Epoch: 20, loss: 42.95673\n",
      "Epoch: 30, loss: 28.42779\n",
      "Epoch: 40, loss: 26.35994\n",
      "48\n",
      "using state SC.csv with id 49\n",
      "(1072, 3)\n",
      "Epoch: 0, loss: 840.85248\n",
      "Epoch: 10, loss: 560.76440\n",
      "Epoch: 20, loss: 481.92490\n",
      "Epoch: 30, loss: 460.05231\n",
      "Epoch: 40, loss: 454.32501\n",
      "49\n",
      "(50, 9)\n"
     ]
    }
   ],
   "source": [
    "params = Parameters()\n",
    "params.num_epochs=50\n",
    "params.num_layers=2\n",
    "params.dropout=0.5\n",
    "Parameters(2, 20, 100, 6, 3, 50, 0.01, 0)\n",
    "\n",
    "columns = ['MAE', 'RMSE', 'MAPE', 'MEAN_MAE', 'MEAN_RMSE', 'MEAN_MAPE', 'NAIVE_MAE', 'NAIVE_RMSE', 'NAIVE_MAPE']\n",
    "df = pd.DataFrame(columns = columns, dtype=np.float64)\n",
    "    \n",
    "for i, state in enumerate(listdir(\"samples/\")):\n",
    "    training_set = readData(\"samples/\"+state, freq=1)\n",
    "    data = Data()\n",
    "    generateData(state, data, params)\n",
    "    #naives  = generateNaive(data, params)\n",
    "    model = train(data, params)\n",
    "    preds, labels = test(model, data, params)\n",
    "    #print(\"----- {} Results -----\".format(state.replace(\".csv\", \"\")))\n",
    "    err_results = print_errors(preds, labels)\n",
    "    #print(a_row.values)\n",
    "\n",
    "    #for naive in naives:\n",
    "    #    err_results.extend(print_errors(naive, labels))   \n",
    "    err_results.extend([0, 0, 0, 0, 0, 0])\n",
    "    \n",
    "    row_df = pd.DataFrame([err_results], index = [state.replace(\".csv\", \"\")], columns = columns, dtype=np.float64)\n",
    "    df = pd.concat([row_df, df])\n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "print(df.shape)\n",
    "df.to_csv(\"lstm-out/concur-{}.csv\".format(params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CPU_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
